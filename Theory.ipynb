{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c681db6",
   "metadata": {},
   "source": [
    "# Text classification with Naive Bayes algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500bbd1c",
   "metadata": {},
   "source": [
    "Starting in the 21st century the rise of social network platform has imposed the problem of fake news spreading. \n",
    "Developing algorithms able to classify short paragraphs of text has become a much relevant research area, in roder to identify for example spam, sexual explicit content and mainly fake news.  \n",
    "There are mainly to approach to solve this task: machine learning classification and **statistical text classification**.\n",
    "In this project we have implemented an algorithm of the second type, the **Naive Bayes**, to try to classify some short tweets and predict the veridicity of them.\n",
    "We have also used the same algorithm to perform the classification of the area of interest of different tweets.\n",
    "In order to improve the performances we have tokenized the text, cleaned each token and finally implemented a technique known as **feature selection**.\n",
    "The detailes of these techniques will be discussed in the follow of this notebook.\n",
    "We start now with a theoretical summary on the matter following ref. [1].\n",
    "\n",
    "\n",
    "[1] C. D. Manning, Chapter 13, Text Classification and Naive Bayes, in Introduction to Information\n",
    "Retrieval, Cambridge University Press, 2008.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085e518d",
   "metadata": {},
   "source": [
    "## The text classification problem\n",
    "\n",
    "In text classification problem we are given a **document space** $\\mathcal{X}$ containing a set of **documents** $d\\in\\mathcal{X}$ and a set of **classes** $\\mathcal{C}=\\{c_1, c_2,..., c_j\\}$ that are tipycally distinguished by a **label**.\n",
    "Given a **training set** $\\mathcal{D}$ the goal is to devolop a **classification function** $\\gamma$ that maps documents to classes:\n",
    "$$\\gamma: \\mathcal{X} \\rightarrow \\mathcal{C}$$\n",
    "\n",
    "Naive Bayes is actually a **supervised learning method** $\\Gamma$ that takes in input a training set and returns a classifier:\n",
    "$$\\Gamma\\left(\\mathcal{D}\\right)=\\gamma$$\n",
    "\n",
    "Once we have learned the classification function $\\gamma$ we can apply it on a **test set** and make prediction about the class of a given document $d$\n",
    "\n",
    "## Naive Bayes tect classification\n",
    "\n",
    "In this section we introduce the **multinomial Naive Bayes** which is the particular algorithm that we have implemented and it is a probabilistic learning method.\n",
    "\n",
    "The probability of a document $d$ being in class $c$ is given by the Bayes theorem:\n",
    "\n",
    "$$P(c|d)\\propto P(C) \\prod_{1\\le k\\le n_d} P(t_k|c)$$\n",
    "\n",
    "where $P(t_k|c)$ is the conditional probability of **term** t_k occurring in a document of class $c$.\n",
    "\n",
    "After computing the posterior probability of each class we choose the class with the highest probability.\n",
    "\n",
    "We note that in the previous equation different conditional probabilities are multiplied, leading to small value to deal with. \n",
    "For this reason we take the logarithm of the probabilities to compute the posterior and only at the end the exponential of the result.\n",
    "\n",
    "Now the problem is to estimate the parameters $P(c)$ and $P(t_k|c)$. \n",
    "The first one is the prior of each class and is therefore estimated as\n",
    "$$ P(c)=\\frac{N_c}{N} $$\n",
    "where $N_c$ is the number of documents in class $c$ and $N$ is the total number of document in the training set.\n",
    "\n",
    "The conditional probability could be naively estimated as\n",
    "$$P(t|c)=\\frac{T_{ct}}{\\sum_{t'\\in\\mathcal{V}}T_{ct'}}$$\n",
    "where $T_{ct}$ is the number of occurences of term $t$ in the training documents of class $c$ and $\\mathcal{V}$ is the vocabulary of the training set. \n",
    "In this case we include multiple occurences of a term in the document making a positional indipendence assumption.\n",
    "The problem with this definition is that in the case that a particular term does not occur in the training data the conditional probability is zero, that being multiplied with the rest of the condtional probabilities and the prior returns a final zero.\n",
    "In order to deal with that we introduce the **Laplace smoothing** that basically consist in countin at least one appearence of each term in the training sent adding a $+1$ both at numerator and denominator:\n",
    "\n",
    "$$P(t|c)=\\frac{T_{tc}+1}{\\sum_{t'\\in\\mathcal{V}}(T_{ct'}+1)}=\\frac{T_{ct}+1}{\\sum_{t'\\in\\mathcal{V}}T_{ct'}+B}$$\n",
    "where $B=|V|$ is the number of terms in the vocabulary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
